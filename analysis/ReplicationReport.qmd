---
title: "Replication of Speaker Production Experiment in Yoon et al. (2020)"
author: "Kim Tien Nguyen"
date: "June 7, 2025"
format:
  html:
    toc: true
    toc_depth: 3
---

## Reference
Yoon, E. J., Tessler, M. H., Goodman, N. D., & Frank, M. C. (2020). Polite Speech Emerges From Competing Social Goals. Open Mind: Discoveries in Cognitive Science, 4, 71–87. 


## Notes about AI use
The code for data preparation, analysis, and visualization included in the sections below was worked out with extensive help from ChatGPT.


## Introduction

Polite speech poses a challenge for assumptions about communication as accurate and efficient information transfer in both classical Gricean pragmatics and more recent probabilistic pragmatics frameworks. For the sake of being polite, speakers opt for utterances that are indirect, underinformative, and sometimes even irrelevant or untruthful. To account for how polite speech arises, Yoon et al. (2020) propose a probabilistic cognitive model which is based on the Rational Speech Act (RSA) framework and incorporates three different informational and social utilities into its machinery. They hypothesize that underlying a speaker’s polite language use is a competition between three goals: to be informative, to be kind (to save the listener’s face), and to present oneself in a good light – quantitatively formalized as three separate weighted utilities (informational, prosocial, and self-presentational) contributing to the total utterance utility. Crucially, the model predictions and behavioral results converge on the following polite speech pattern: when describing a bad state, a speaker whose goal is to be both informative and kind would produce more indirect, negated utterances - saying, for example, that something is "not bad" or "not terrible" even though they actually feel much more negatively about it. This replication project aims to reproduce this key finding in Yoon et al.'s (2020) production experiment.


## Methods

### Power Analysis

Yoon et al. (2020) analyzed behavioral data from 202 participants in their production experiment using a Bayesian mixed-effects model. The model revealed strong interaction effects between true state and the speaker's goal with tight 95% Bayesian credible intervals that did not include zero: In worse states, speakers whose goal was to be both informative and kind produced more negation than speakers whose goal was only to be informative (posterior mean M = −1.33, with 95% Bayesian CI of [−1.69, −0.98]) or only to be kind (M = −0.50, [−0.92, −0.07]). 

### Planned Sample

Given those statistically robust effects found in the original study with 202 participants and the relatively low number of trials per participant, I plan to use a matching sample size of 202. Using the same sample size would also facilitate a more direct, one-to-one comparison of the observed effects between the original and replication study.

### Materials

The experiment has a 4x3 factorial design: true state (0, 1, 2, 3 hearts) and speaker’s goal (informative, kind, both). Each participant is given 12 context items depicting each of the 12 conditions in randomized order. In every context item, the speaker is supposed to tell the listener what they think about something the listener created. The list from which the listener's creative work is drawn for each context item includes: "poem","cake","presentation","song","film","solo","monologue","dance","painting","app","review", "recital". The speaker's goal and task prompt for participants are given in the form of a conditional question that reads: “If [speaker's name] wanted to make [listener's name] feel good but not necessarily give accurate and informative feedback (or give accurate and informative feedback but not necessarily make [listener's name] feel good, or BOTH make [listener's name] feel good AND give accurate and informative feedback), what would [speaker's name] be most likely to say?”

### Procedure	

First, participants read a scenario where person A doesn’t know how good their creative work is and asks for person B’s opinion on it. Then, they are given 2 pieces of information: (i) how B truly feels about A’s creative work shown as a number of red hearts, and (ii) what B’s goal in communicating an answer to A is
Finally, they are asked to indicate what person B would most likely say given those two pieces of information. Formula for indicating person B’s most likely production: It + was/wasn’t (chosen from a dropdown menu) + terrible/bad/good/amazing (chosen from a dropdown menu). 

### Analysis Plan
Participants who only give the exact same answer for every scenario and/or take less than 3 minutes to complete the experiment will be excluded from analysis (the 3-minute minimum estimation is based on pilot B results).

Following Yoon et al.'s (2020) analysis scripts, I plan to analyze both my replication data and the original data as follows.
(All data and analysis scripts are made publicly available by the authors in this Github repository: "https://github.com/ejyoon/polite_speaker")

The preprocessed dataset of my replication data will have the exact same data structure as the one provided by the authors (titled "speaker_production.csv" in their Github repository), with the following columns: 
- subid (unique participant ids), trial (1-12 for 12 trials)	
- item (one of the 12 context items listed above under "Materials")	
- goal ("both","informative", "social")	
- true_state ("heart0", "heart1", "heart2", "heart3")
- positivity ("neg", "no_neg" corresponding to 'wasn't' and 'was' respectively in participants' responses)
- utterance ("terrible", "bad", "good", "amazing")

The analysis will include a generalized linear mixed-effects model and a Bayesian mixed-effects model. Below are the model formulas and their components.

Formula for generalized linear mixed-effects model (using glmer() from lme4): 
positivity ~ true_state * goal * (1 |subid); 

Formula for Bayesian mixed-effects model (using brm from brms): 
positivity ~ true_state * goal + (true_state + goal |subid) + (true_state + goal | item);

positivity will be coded as a numeric vector with 0 corresponding to "neg" (saying "It wasn't..."), and 1 corresponding to "no_neg" (saying "It was..."); 

true_state will be coded as a numeric vector, ranging from 0-3 for use in glmer() and standardized using scale() for use in brm(); 

goal will be coded as a factor with three levels, with "both" being the reference one.

The key analysis of interest is the interaction effects between true state and goal in the Bayesian mixed-effects models.

### Differences from Original Study

The participants in the sample are recruited on Prolific instead of Amazon’s Mechanical Turk. The experiment looks somewhat aesthetically different from the original experiment (e.g., color and positioning of "next" button, color of progress bar, etc.), but it is still the same in function and logic (no difference in procedure or task). The original list of context items has 13 items (plus "cookie"), but it was reduced to 12 (leaving out "cookie") for the 12 scenarios due to concern about having two similar scenarios involving baking ("cake" and "cookie"). The authors did not report a generalized linear mixed-effects model in their paper, but they included it in their analysis. 


### Methods Addendum (Post Data Collection)

#### Actual Sample
The sample includes data from 202 participants (2 from pilot B + 200 in subsequent sample since there was no difference in how the experiment was implemented and how the relevant data were collected after pilot B). There were 203 participants registered on Prolific in the post-pilot B sample, but only 200 data files could be found in the OSF repository - possibly due to technical issues in the data saving pipeline for the 3 missing participants. Filter criteria for the participant pool on Prolific were: country of residence is the US, UK, or Canada; first, primary and fluent languages are all English; approval rate is 100%.

#### Differences from pre-data collection methods plan
202 is the sample size reported in Yoon et al. (2020) –  however, I discovered during analysis that the preprocessed combined data file shared by the authors ("speaker_production.csv") contains 203 participants and I couldn’t find an explanation on how 1 participant might have been excluded. I ran the analysis on this original data file with 203 participants and the replication data file with 202 participants.

Due to unresolved problems with installing RBuildTools/rstan/brms, I ran the Bayesian mixed-effects model using stan_glmer() from rstanarm instead. This difference inevitably makes a 1-to-1 comparison between my analysis and the authors' impossible. However, using stan_glmer() on the original data file still reproduces relatively close values that reflect the same trends in fixed effects as reported in the paper.


## Results

### Data preparation

Data preparation following the analysis plan.

#### Set up

```{r setup, include=FALSE}
# Install & load required packages
required_pkgs <- c(
  "readr", "dplyr","tidyr",  "stringr", "rstan",
  "rstanarm", "broom.mixed", "ggplot2", "patchwork", "lme4"
)
new_pkgs <- setdiff(required_pkgs, rownames(installed.packages()))
if (length(new_pkgs)) install.packages(new_pkgs)

library(readr)
library(stringr)
library(rstanarm)     # stan_glmer
library(broom.mixed)  # tidy()
library(lme4)
library(patchwork)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(forcats)

# Use all cores for sampling
options(mc.cores = parallel::detectCores())

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 6,
  fig.height = 4
)
```


#### Create combined file of replication data 

```{r create-replication_data_csv}
# (A) Set wd to source file location ("analysis" folder) and point to "full sample" folder
data_dir <- "../data/full sample"

# (B) Grab a sorted list of all CSVs in "full sample" folder
files <- list.files(
  path       = data_dir,
  pattern    = "\\.csv$",
  full.names = TRUE
  )

# (C) Read + process + bind
replication_data <- map_dfr(
  .x   = seq_along(files),
  .f   = function(i) {
    read_csv(files[i], col_types = cols()) %>%      # read file #i
      transmute(
        subid       = i,
        trial,
        item,
        goal,
        true_state,
        positivity = recode(positivity,"was" = "no_neg", "wasn't" = "neg"),
        utterance
      )
    }
  )

# (D) Write out the combined CSV with all 202 participants
write_csv(replication_data, "../data/replication_data.csv")
```

#### Load and prepare data

##### Load and prepare original data

```{r prepare-original-data}
###### Load ###### 
# The original "speaker_production.csv" file provided by the authors was downloaded, 
# renamed as "original_data.csv" for ease of reference, and 
# saved in the same "data" folder as the preprocessed "replication_data.csv" file above
og_raw  <- read_csv("../data/original_data.csv",   col_types = cols())

###### Prepare ###### 
og_data <- og_raw %>%
  mutate(
    # (A) Make positivity numeric (same code as the authors' for their glmer and brm functions):
    #     Factor levels in alphabetical order by default: 
    #     Level 1: "neg" (saying “It wasn’t …”) → positivity = 0
    #     Level 2: "no_neg" (saying “It was …”) → positivity = 1
    positivity = factor(positivity, labels = c(0,1)), 
    positivity = as.numeric(as.character(positivity)),
    
    # (B) Scale true_state the way the authors did (zero‐center + divide by sd):
    #     raw column: "heart0", "heart1", "heart2", "heart3"
    true_state = as.numeric(substr(true_state, 6, 6)),   # 0,1,2,3
    true_state = scale(true_state),                  # zero‐mean, unit‐SD

    # (C) Force “both” to be the reference level in goal:
    goal = factor(goal, levels = c("both", "informative", "social")),

    # (D) Make subid and item factors
    subid = factor(subid),
    item  = factor(item),
    ) %>%
  select(
    subid, trial, item,
    goal, true_state, 
    positivity, # Now  “It wasn't …” = 0, “It was …” = 1,
    utterance,
  )

summary(og_data$true_state)   # mean ≈ 0, sd ≈ 1
```

##### Load and prepare replication data

```{r prepare-replication-data}
###### Load ###### 
rep_raw <- read_csv("../data/replication_data.csv",     col_types = cols())

###### Prepare ###### 
rep_data <- rep_raw %>%
  mutate(
    # (A) Make positivity numeric (same code as the authors' for their glmer and brm functions):
    #     Factor levels in alphabetical order by default: 
    #     Level 1: "neg" (saying “It wasn’t …”) → positivity = 0
    #     Level 2: "no_neg" (saying “It was …”) → positivity = 1
    positivity = factor(positivity, labels = c(0,1)), 
    positivity = as.numeric(as.character(positivity)),
    
    # (B) Scale true_state the way the authors did (zero‐center + divide by sd):
    #     raw column: "heart0", "heart1", "heart2", "heart3"
    true_state = as.numeric(substr(true_state, 6, 6)),   # 0,1,2,3
    true_state = scale(true_state),                  # zero‐mean, unit‐SD

    # (C) Force “both” to be the reference level in goal:
    goal = factor(goal, levels = c("both", "informative", "social")),

    # (D) Make subid and item factors
    subid = factor(subid),
    item  = factor(item),
    ) %>%
  select(
    subid, trial, item,
    goal, true_state, 
    positivity, # Now  “It wasn't …” = 0, “It was …” = 1,
    utterance,
  )

summary(rep_data$true_state)   # mean ≈ 0, sd ≈ 1
```


### Confirmatory analysis

The analyses as specified in the analysis plan. 

#### Plots

##### Comparing crucial heart0 x both condition in original & replication study

```{r plot-heart0-both-styled, echo=TRUE, warning=FALSE, message=FALSE, fig.width=8, fig.height=4}
 
# ─────────────────────────────────────────────────────────────────────────────
# 1. Summarization function (exactly “heart0 × both” subset):
# ─────────────────────────────────────────────────────────────────────────────
summarize_panel <- function(df) {
  df %>%
    filter(true_state == "heart0", goal == "both") %>%
    mutate(
      positivity = factor(positivity, levels = c("no_neg","neg")),
      utterance  = factor(utterance,
                          levels = c("terrible","bad","good","amazing"))
    ) %>%
    count(utterance, positivity) %>%
    ungroup() %>%
    mutate(
      total_n = sum(n),
      prop    = n / total_n,
      se      = sqrt(prop * (1 - prop) / total_n)
    )
}

# 2. Create summary data frames for original vs replication:
og_df  <- summarize_panel(og_raw)
rep_df <- summarize_panel(rep_raw)

# ─────────────────────────────────────────────────────────────────────────────
# 3. “make_panel()” factory: exactly match the 12‐panel style
# ─────────────────────────────────────────────────────────────────────────────
make_panel <- function(df, panel_title) {
  ggplot(df, aes(x = utterance,
                 y = prop,
                 color = positivity,
                 group = positivity)) +
    # ─── Lines, points, error bars ────────────────────────────────────────────
    geom_line(size = 1) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymin = prop - 1.96*se,
                      ymax = prop + 1.96*se),
                  width     = 0.08,
                  linewidth = 0.6) +
    # ─── “Chance” line at 0.125 (dashed gray) ───────────────────────────────
    geom_hline(yintercept = 0.125,
               color     = "gray50",
               linetype  = "dotted",
               linewidth = 0.5) +
    # ─── Color scale: blue = “no_neg” / red = “neg” ─────────────────────────────
    scale_color_manual(
      values = c("no_neg" = "#1F77B4",
                 "neg"    = "#D62728"),
      labels = c("no_neg" = "It was …",
                 "neg"    = "It wasn't …"),
      guide  = guide_legend(order          = 1,
                            label.position = "bottom",
                            title          = NULL)
    ) +
    # ─── Y axis 0 → 1.0 plus tiny padding above/below ─────────────────────────
    scale_y_continuous(
      limits = c(0, 1.02),
      breaks = seq(0, 1.0, by = 0.25),
      expand = expansion(mult = c(0.0, 0.0))
    ) +
    # ─── Labels & titles ───────────────────────────────────────────────────────
    labs(
      title = panel_title,
      x     = "Utterance",
      y     = "Proportion chosen",
      color = NULL
    ) +
    # ─── Precisely match “white panel + black border + light grid” style ───────
    theme_minimal(base_size = 12) +
    theme(
      # Only major gridlines (light gray); no minor grid
      panel.grid.minor      = element_blank(),
      panel.grid.major      = element_line(color = "gray90", linewidth = 0.3),
      # White background, black border
      panel.background      = element_blank(),
      panel.border          = element_rect(fill   = NA,
                                          color  = "black",
                                          linewidth = 0.7),
      # Angled x‐labels exactly as in 12‐panel
      axis.text.x           = element_text(angle = 30,
                                           vjust = 0.7,
                                           size  = 9),
      axis.text.y           = element_text(size = 9),
      # Center title
      plot.title            = element_text(hjust = 0.5),
      # Legend at bottom
      legend.position       = "bottom",
      legend.key.size       = unit(0.7, "line"),
      legend.text           = element_text(size = 10),
      # Tight margins for compact look
      plot.margin           = margin(5, 5, 5, 5)
    )
}

# ─────────────────────────────────────────────────────────────────────────────
# 4. Build the two panels side by side, sharing a single legend
# ─────────────────────────────────────────────────────────────────────────────
p1 <- make_panel(og_df,  "Original")
p2 <- make_panel(rep_df, "Replication")

combined <- (p1 + p2) +
  plot_layout(ncol = 2, guides = "collect") &
  theme(legend.position = "bottom")

# ─────────────────────────────────────────────────────────────────────────────
# 5. Render
# ─────────────────────────────────────────────────────────────────────────────
print(combined)

```

##### Comparing all 12 conditions in original & replication study 

```{r plot‐12cells‐with‐right‐strips, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=7}

# ------------------------------------------------------------------------------
# (1) A helper to summarize raw data into n / prop / SE, tagging each row with
#     a “source” label (Original vs Replication).  We force the factor‐levels
#     for true_state, goal, positivity, and utterance to be exactly as in your
#     single‐cell code, so that faceting is consistent.
# ------------------------------------------------------------------------------
make_summary <- function(df, source_label) {
  df %>%
    mutate(
      true_state = fct_relevel(true_state, "heart0", "heart1", "heart2", "heart3"),
      goal       = fct_relevel(goal,       "both",   "informative", "social"),
      positivity = factor(positivity,      levels = c("no_neg", "neg")),
      utterance  = factor(utterance,       levels = c("terrible", "bad", "good", "amazing")),
      source     = source_label
    ) %>%
    # Count each combination of (true_state × goal × utterance × positivity):
    group_by(true_state, goal, utterance, positivity, source) %>%
    summarise(n = n(), .groups = "drop") %>%
    # Ensure we have a row for positivity="no_neg" & "neg" even if n=0:
    complete(
      true_state, goal, utterance,
      positivity = c("no_neg", "neg"),
      source     = source_label,
      fill       = list(n = 0L)
    ) %>%
    # Within each (true_state × goal × source), compute prop and se:
    group_by(true_state, goal, source) %>%
    mutate(
      total_n = sum(n),
      prop    = if_else(total_n > 0L, n / total_n, 0),
      se      = if_else(total_n > 0L,
                        sqrt(prop * (1 - prop) / total_n),
                        0)
    ) %>%
    ungroup()
}

# Build two summaries: one for the original data, one for your replication:
og_summary  <- make_summary(og_raw,  "Original")
rep_summary <- make_summary(rep_raw, "Replication")

# ------------------------------------------------------------------------------
# (2) Now draw a single ggplot with 3 rows × 4 columns. 
#     • Plot the “Original” data first (dashed, thin, open circles)
#     • Then overlay the “Replication” data (solid, thicker, filled circles)
#     • Use facet_grid(..., switch = "y") + strip.placement = "outside"
#       so that the goal labels appear on the right
# ------------------------------------------------------------------------------
ggplot() +
  # ─── (2a) Plot ORIGINAL data (dashed, thin, open circles) ─────────────────
  geom_line(
    data = og_summary,
    aes(
      x        = utterance,
      y        = prop,
      color    = positivity,
      group    = interaction(positivity, source),
      linetype = source
    ),
    size = 0.5   # thinner than the replication lines
  ) +
  geom_point(
    data = og_summary,
    aes(
      x        = utterance,
      y        = prop,
      color    = positivity,
      group    = interaction(positivity, source),
      shape    = source
    ),
    size   = 2,
    stroke = 0.8  # open‐circle style
  ) +
  geom_errorbar(
    data = og_summary,
    aes(
      x        = utterance,
      ymin     = prop - 1.96 * se,
      ymax     = prop + 1.96 * se,
      color    = positivity,
      group    = interaction(positivity, source),
      linetype = source
    ),
    width     = 0.08,
    linewidth = 0.3  # half of replication’s 0.6
  ) +

  # ─── (2b) Plot REPLICATION data on top (solid, thicker, filled circles) ─────
  geom_line(
    data = rep_summary,
    aes(
      x        = utterance,
      y        = prop,
      color    = positivity,
      group    = interaction(positivity, source),
      linetype = source
    ),
    size = 1     # thicker than the original lines
  ) +
  geom_point(
    data = rep_summary,
    aes(
      x        = utterance,
      y        = prop,
      color    = positivity,
      group    = interaction(positivity, source),
      shape    = source
    ),
    size = 2
  ) +
  geom_errorbar(
    data = rep_summary,
    aes(
      x        = utterance,
      ymin     = prop - 1.96 * se,
      ymax     = prop + 1.96 * se,
      color    = positivity,
      group    = interaction(positivity, source),
      linetype = source
    ),
    width     = 0.08,
    linewidth = 0.6
  ) +

  # ─── (2c) Chance‐level horizontal line at y = 0.125 (dashed gray) ───────────
  geom_hline(
    yintercept = 0.125,
    color      = "gray50",
    linetype   = "dashed",
    linewidth  = 0.5
  ) +

  # ─── (2d) Color scale for positivity: blue = “no_neg”, red = “neg” ──────────
  scale_color_manual(
    values = c("no_neg" = "#1F77B4",
               "neg"    = "#D62728"),
    labels = c("no_neg" = "It was …",
               "neg"    = "It wasn't …"),
    guide  = guide_legend(
                order          = 1,
                title          = NULL,
                label.position = "bottom"
             )
  ) +

  # ─── (2e) Line‐type scale for “Source”: dashed = Original, solid = Replication ──
  scale_linetype_manual(
    name   = "Source",
    values = c("Original"    = "dashed",
               "Replication" = "solid"),
    guide  = guide_legend(
                order          = 2,
                title.position = "top",
                title.hjust    = 0.5
             )
  ) +

  # ─── (2f) Shape scale for “Source”: open circle for Original, filled for Replication ─
  scale_shape_manual(
    name   = "Source",
    values = c("Original"    = 1,   # open circle
               "Replication" = 16), # filled circle
    guide  = guide_legend(
                order          = 2,
                title.position = "top",
                title.hjust    = 0.5
             )
  ) +

  # ─── (2g) Y‐axis goes from 0 to 1.02 with tick marks every .25 ──────────────
  scale_y_continuous(
    limits = c(0, 1.02),
    breaks = seq(0, 1.0, by = 0.25),
    expand = expansion(mult = c(0.01, 0.01))
  ) +

  # ─── (2h) Facet into 3 rows = goal, 4 columns = true_state, switch Y to move
  #          the row labels (“Both,” “Informative,” “Kind”) to the right. ─────────
  facet_grid(
    rows     = vars(goal),
    cols     = vars(true_state),
    switch   = "y",
    labeller = labeller(
      true_state = c(
        "heart0" = "0 heart",
        "heart1" = "1 heart",
        "heart2" = "2 hearts",
        "heart3" = "3 hearts"
      ),
      goal       = c(
        "both"        = "Both",
        "informative" = "Informative",
        "social"      = "Kind"
      )
    )
  ) +

  # ─── (2i) Axis labels ─────────────────────────────────────────────────────────
  labs(
    x     = "Utterance",
    y     = "Proportion chosen"
  ) +

  # ─── (2j) Clean theme: white background, thin black border, light gray grid ───
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.minor      = element_blank(),
    panel.grid.major      = element_line(color = "gray90", linewidth = 0.3),
    panel.background      = element_blank(),
    panel.border          = element_rect(fill   = NA,
                                        color  = "black",
                                        linewidth = 0.7),

    panel.spacing.x       = unit(0.5, "cm"),
    panel.spacing.y       = unit(0.5, "cm"),

    # Hide the left‐side strip text; style only the right‐side strip text:
    strip.text.y.right    = element_text(
                              
                              size   = 10,
                              margin = margin(l = 6)
                            ),

    # Style the top strip (true_state headings) in bold, with a bit of bottom margin
    strip.text.x.top      = element_text(
                              
                              size   = 10,
                              margin = margin(b = 6)
                            ),

    # Slanted X‐axis labels for readability
    axis.text.x           = element_text(angle  = 30,
                                         vjust  = 0.7,
                                         size   = 9),
    axis.text.y           = element_text(size = 9),

    # Tight plot margins
    plot.margin           = margin(5, 5, 5, 5),

    # Put both legends (color & linetype/shape) at the bottom, in one row
    legend.position       = "bottom",
    legend.box            = "horizontal",
    legend.key.size       = unit(0.7, "line"),
    legend.text           = element_text(size = 10)
  )
```

#### Frequentist analysis: generalized linear mixed-effects models 

##### Original frequentist model

```{r original-frequentist-model}
og_glmer <- og_raw %>%
  mutate(
    positivity = factor(positivity, labels = c(0,1)), 
    positivity = as.numeric(as.character(positivity)),
    true_state = as.numeric(substr(true_state, 6, 6)),   # 0,1,2,3
    goal = factor(goal, levels = c("both", "informative", "social"))
    )   

summary(glmer(data=og_glmer, positivity ~ true_state * goal * (1 |subid), family=binomial)) 
# authors' note in their code for this: fuller random effect structure causes model convergence failure
```

##### Replication frequentist model

```{r replication-frequentist-model}
rep_glmer <- rep_raw %>%
  mutate(
    positivity = factor(positivity, labels = c(0,1)), 
    positivity = as.numeric(as.character(positivity)),
    true_state = as.numeric(substr(true_state, 6, 6)),   # 0,1,2,3
    goal = factor(goal, levels = c("both", "informative", "social"))
    )  

summary(glmer(data=rep_glmer, positivity ~ true_state * goal * (1 |subid), family=binomial)) 
```

#### Bayesian analysis: Bayesian mixed-effects model 

##### Original Bayesian model

```{r original-Bayesian-model}
###### Fit the model ###### 

fit_og <- stan_glmer(
  positivity ~ true_state * goal
           + (true_state + goal | subid)
           + (true_state + goal | item),

  data            = og_data,         
  family          = binomial(link = "logit"),

  # (A) Fixed‐effects priors: Normal(0,1) exactly as in brms
  prior           = normal(0, 1),    # class = "b"
  prior_intercept = normal(0, 1),    # class = "Intercept"

  # (B) Random‐effects priors: approximate brm's default Student-t(3,0,2.5) on each SD via decov
  #    decov(regularization=6.25, concentration=1)
  prior_covariance = decov(
    regularization = 6.25,  # scale = sqrt(6.25) = 2.5  (correct scale)
    concentration  = 1      # LKJ(η=1) on correlation
  ),

  # (C) MCMC settings (match brms defaults)
  chains      = 4,
  iter        = 2000,   # 1000 warmup + 1000 sampling
  warmup      = 1000,
  thin        = 1,
  adapt_delta = 0.8,
  seed        = 1234
)

# Print the fixed‐effects summary
print(fit_og, digits = 2)


###### Clean Fixed‐Effects Summary Table ###### 
fixed_og_tbl <- tidy(
  fit_og,
  effects  = "fixed",
  conf.int = TRUE
) %>%
  rename(
    Predictor     = term,
    Mean          = estimate,
    SD            = std.error,
    `95% CI-Lower` = conf.low,
    `95% CI-Upper` = conf.high
  ) %>%
  mutate(
    Predictor = recode(
      Predictor,
      `(Intercept)`                = "Intercept",
      `true_state`                 = "True state",
      `goalinformative`            = "Goal: Informative",
      `goalsocial`                 = "Goal: Kind",
      `true_state:goalinformative` = "True state * Informative",
      `true_state:goalsocial`      = "True state * Kind"
    )
  ) %>%
  select(Predictor, Mean, SD, `95% CI-Lower`, `95% CI-Upper`)

knitr::kable(
  fixed_og_tbl,
  digits  = 2,
  caption = "Table 1 (Original): Posterior Summary for Fixed Effects"
)
```

##### Replication Bayesian model

```{r replication-Bayesian-model}
###### Fit the model ###### 

fit_rep <- stan_glmer(
  positivity ~ true_state * goal
           + (true_state + goal | subid)
           + (true_state + goal | item),

  data            = rep_data,         
  family          = binomial(link = "logit"),

  # (A) Fixed‐effects priors: Normal(0,1) exactly as in brms
  prior           = normal(0, 1),    # class = "b"
  prior_intercept = normal(0, 1),    # class = "Intercept"

  # (B) Random‐effects priors: approximate brm's default Student-t(3,0,2.5) on each SD via decov
  #    decov(regularization=6.25, concentration=1)
  prior_covariance = decov(
    regularization = 6.25,  # scale = sqrt(6.25) = 2.5  (correct scale)
    concentration  = 1      # LKJ(η=1) on correlation
  ),

  # (C) MCMC settings (match brms defaults)
  chains      = 4,
  iter        = 2000,   # 1000 warmup + 1000 sampling
  warmup      = 1000,
  thin        = 1,
  adapt_delta = 0.8,
  seed        = 1234
)

# Print the fixed‐effects summary
print(fit_rep, digits = 2)


###### Clean Fixed‐Effects Summary Table ###### 
fixed_rep_tbl <- tidy(
  fit_rep,
  effects  = "fixed",
  conf.int = TRUE
) %>%
  rename(
    Predictor     = term,
    Mean          = estimate,
    SD            = std.error,
    `95% CI-Lower` = conf.low,
    `95% CI-Upper` = conf.high
  ) %>%
  mutate(
    Predictor = recode(
      Predictor,
      `(Intercept)`                = "Intercept",
      `true_state`                 = "True state",
      `goalinformative`            = "Goal: Informative",
      `goalsocial`                 = "Goal: Kind",
      `true_state:goalinformative` = "True state * Informative",
      `true_state:goalsocial`      = "True state * Kind"
    )
  ) %>%
  select(Predictor, Mean, SD, `95% CI-Lower`, `95% CI-Upper`)

knitr::kable(
  fixed_rep_tbl,
  digits  = 2,
  caption = "Table 2 (Replication): Posterior Summary for Fixed Effects"
)
```


### Exploratory analyses

Any follow-up analyses desired (not required).  


## Discussion

### Summary of Replication Attempt

The results support the behavioral hypothesis by Yoon et al. (2020): When describing bad states, a speaker who aims to be both informative and kind would produce more indirect utterances containing negation (e.g., "It wasn't terrible") than a speaker who aims to be informative or kind only. This can be concluded from both a frequentist and a Bayesian statistical analysis showing significant interaction effects between true state and goal. Crucially, the Bayesian mixed-effects model of the replication data show similar interaction effects as the model of the original data reported in Yoon et al., (2020): In worse true state conditions, a speaker with the goal to be both informative and kind produced more negated utterances than a speaker with the goal to be informative only (M = −1.74, 95% Bayesian CI: [−1.99, −1.49]) or kind only (M = −1.02, 95% Bayesian CI: [−1.28, −0.75]). In sum, this replication study successfully replicated the result of the original study.

### Commentary

While the results quite closely match the patterns found in Yoon et al. (2020), there are some noticeable differences in the 3 hearts x kind and 2-3 hearts x informative conditions. Participants in the replication study did not overwhelmingly choose "It was amazing" to describe 3-hearts states when trying to be informative or kind only (as found in the original study), and they even chose "It wasn't bad" above chance level to describe 2-heart states when aiming to be informative only. This suggests that the preference for affirmative utterances in better/truly positive states given the goal to be only informative or kind might be more graded than observed in the original study. One possible explanation for the differences in positivity described above is that people may vary in how confident they are about the objective validity of the given true states (they may question: "Speaker B thinks X but is X objectively true?") and thus prefer to leave some room for uncertainty by not opting for a too positive utterance.

